{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests black nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, out, force=False, verify=True):\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if force:\n",
    "        print(f\"Removing file at {str(out)}\")\n",
    "        out.unlink()\n",
    "\n",
    "    if out.exists():\n",
    "        print(\"File already exists.\")\n",
    "        return\n",
    "    print(f\"Downloading {url} at {str(out)} ...\")\n",
    "    # open in binary mode\n",
    "    with out.open(mode=\"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url, verify=verify)\n",
    "        for chunk in response.iter_content(100000):\n",
    "            # write to file\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_VALUE = [\"Unkn0wnV@lue\"]\n",
    "\n",
    "\n",
    "class SafeLabelEncoder(LabelEncoder):\n",
    "    \"\"\"\n",
    "    Safe label encoder, encoding every unknown value as Unkn0wnV@lue.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, y):\n",
    "        \"\"\"\n",
    "        Fit the label encoder, by casting the numpy array as a string, then adding the code for unknown.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        SafeLabelEncoder\n",
    "            itself, fitted\n",
    "        \"\"\"\n",
    "        return super().fit(np.concatenate((y.astype(\"str\"), UNKNOWN_VALUE)))\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        \"\"\"\n",
    "        Fit the encoder, then transform the input data and returns it.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy array\n",
    "            the encoded data\n",
    "        \"\"\"\n",
    "        self.fit(y)\n",
    "        return super().transform(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "        \"\"\"\n",
    "        Transform the input data and returns it.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            the values to fit\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy array\n",
    "            the encoded data\n",
    "        \"\"\"\n",
    "        return super().transform(\n",
    "            np.where(\n",
    "                np.isin(y.astype(\"str\"), self.classes_), y.astype(\"str\"), UNKNOWN_VALUE\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download census-income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "url_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "\n",
    "dataset_name = \"census-income\"\n",
    "out = Path(os.getcwd() + \"/data/\" + dataset_name + \".csv\")\n",
    "out_test = Path(os.getcwd() + \"/data/\" + dataset_name + \"_test.csv\")\n",
    "\n",
    "download(url, out, force=True)\n",
    "download(url_test, out_test, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"target\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(out, names=cols)\n",
    "test = pd.read_csv(out_test, names=cols, skiprows=2)\n",
    "target = \"target\"\n",
    "\n",
    "train[target] = train[target].str.strip()\n",
    "# Test has . in label, let's clean it\n",
    "test[target] = test[target].str.strip().str.strip(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Set\" not in train.columns:\n",
    "    train[\"Set\"] = np.random.choice(\n",
    "        [\"train\", \"valid\"], p=[0.8, 0.2], size=(train.shape[0],)\n",
    "    )\n",
    "\n",
    "train_indices = train[train.Set == \"train\"].index\n",
    "valid_indices = train[train.Set == \"valid\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_columns = list(set(train.columns.tolist()) - set([target]) - set([\"Set\"]))\n",
    "used_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "Label encode categorical features and fill empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique = train[used_columns].nunique()\n",
    "types = train[used_columns].dtypes\n",
    "\n",
    "cat_cols = train[used_columns].columns[(nunique < 200) | (types == \"object\")]\n",
    "other_cols = train[used_columns].columns[~train[used_columns].columns.isin(cat_cols)]\n",
    "print(cat_cols)\n",
    "print(other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique[\"education\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna\n",
    "train[cat_cols] = train[cat_cols].astype(\"str\")\n",
    "train[other_cols] = train[other_cols].fillna(train[other_cols].mean())\n",
    "\n",
    "test[cat_cols] = test[cat_cols].astype(\"str\")\n",
    "test[other_cols] = test[other_cols].fillna(train[other_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = {}\n",
    "for col in cat_cols:\n",
    "    label_enc = SafeLabelEncoder()\n",
    "    enc[col] = label_enc\n",
    "    train[col] = label_enc.fit_transform(train[col])\n",
    "    test[col] = label_enc.transform(test[col])\n",
    "enc[target] = SafeLabelEncoder()\n",
    "train[target] = enc[target].fit_transform(train[target])\n",
    "test[target] = enc[target].transform(test[target])\n",
    "\n",
    "enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define categorical features for categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_feat = [\"Set\"]\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(used_columns) if f in cat_cols]\n",
    "cat_dims = [len(enc[f].classes_) for f in used_columns if f in cat_cols]\n",
    "print(cat_idxs)\n",
    "print(cat_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[used_columns].values\n",
    "y = train[target].values\n",
    "\n",
    "X_test = test[used_columns].values\n",
    "y_test = test[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = os.cpu_count() if torch.cuda.is_available() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetTuner(TabNetClassifier):\n",
    "    def fit(self, X, y, *args, **kwargs):\n",
    "        # Dirty trick => would be better to add n_d in grid, or fix it in __init__ of tuner\n",
    "        self.n_d = self.n_a\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=0, shuffle=True, stratify=y\n",
    "        )\n",
    "        return super().fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            patience=20,\n",
    "            X_valid=X_valid,\n",
    "            y_valid=y_valid,\n",
    "            num_workers=num_workers,\n",
    "            max_epochs=1000,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TabNetTuner(cat_idxs=cat_idxs, cat_dims=cat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate embedding size based on cat dims\n",
    "cat_emb_dim_list = []\n",
    "for max_dim in [1, 5, 10, 20, 50]:\n",
    "    cat_emb_dim_list.append([min(nb // 2, max_dim) for nb in cat_dims])\n",
    "cat_emb_dim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"n_a\": [3, 5, 8, 13, 21],\n",
    "    # \"n_d\": [8], #\n",
    "    \"cat_emb_dim\": cat_emb_dim_list,\n",
    "    \"n_independent\": [0, 1, 2, 5],\n",
    "    \"n_shared\": [0, 1, 2],\n",
    "    \"n_steps\": [1, 3, 5, 8],\n",
    "    \"clip_value\": [1],\n",
    "    \"gamma\": [0.5, 1.3, 3],\n",
    "    \"momentum\": [0.1, 0.05, 0.02, 0.005],\n",
    "    \"lambda_sparse\": [0.1, 0.01, 0.001],\n",
    "    \"lr\": [0.1, 0.02, 0.001],\n",
    "    \"verbose\": [1],\n",
    "    # optimizer_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(\n",
    "    clf,\n",
    "    grid,\n",
    "    n_iter=90,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=1,\n",
    "    iid=False,\n",
    "    refit=False,\n",
    "    cv=[(train_indices, valid_indices)],\n",
    "    verbose=1,\n",
    "    pre_dispatch=0,\n",
    "    random_state=0,\n",
    "    return_train_score=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[used_columns].values\n",
    "y = train[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X, y)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After searching for best params, we need to retrain a model, we chose a make that on 5 folds.\n",
    "{'verbose': 0,\n",
    " 'n_steps': 1,\n",
    " 'n_shared': 2,\n",
    " 'n_independent': 0,\n",
    " 'n_a': 21,\n",
    " 'momentum': 0.02,\n",
    " 'lr': 0.1,\n",
    " 'lambda_sparse': 0.01,\n",
    " 'gamma': 0.5,\n",
    " 'clip_value': 1,\n",
    " 'cat_emb_dim': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    " {'verbose': 0,\n",
    " 'n_steps': 5,\n",
    " 'n_shared': 2,\n",
    " 'n_independent': 1,\n",
    " 'n_a': 5,\n",
    " 'momentum': 0.02,\n",
    " 'lr': 0.1,\n",
    " 'lambda_sparse': 0.001,\n",
    " 'gamma': 0.5,\n",
    " 'clip_value': 1,\n",
    " 'cat_emb_dim': [5, 3, 5, 1, 5, 5, 3, 5, 4, 5, 5, 5, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for train_index, valid_index in skf.split(X, y):\n",
    "    clf = TabNetClassifier(cat_idxs=cat_idxs, cat_dims=cat_dims, **search.best_params_)\n",
    "    clf.fit(\n",
    "        X[train_index],\n",
    "        y[train_index],\n",
    "        patience=20,\n",
    "        X_valid=X[valid_index],\n",
    "        y_valid=y[valid_index],\n",
    "    )\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros(shape=y_test.shape)\n",
    "for model in models:\n",
    "    preds += clf.predict_proba(X_test)[:, 1]\n",
    "preds = preds / len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auc = roc_auc_score(y_score=preds, y_true=y_test)\n",
    "\n",
    "print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
